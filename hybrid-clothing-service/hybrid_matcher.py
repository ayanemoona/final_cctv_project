# hybrid_matcher.py
import cv2
import numpy as np
import torch
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
from sklearn.metrics.pairwise import cosine_similarity
import json
import base64
import io
from typing import Dict, List, Any
import logging

logger = logging.getLogger(__name__)

class HybridClothingMatcher:
    """
    Computer Vision + MobileNet ê²°í•©í•œ ì˜·ì°¨ë¦¼ ë§¤ì¹­ ì‹œìŠ¤í…œ
    ê²½ëŸ‰í™”ë˜ê³  ë°°í¬ ì•ˆì •ì ì¸ ì†”ë£¨ì…˜
    """
    
    def __init__(self):
        self.registered_suspects = {}
        self.setup_mobilenet()
        logger.info("ğŸ¯ Hybrid Clothing Matcher ì´ˆê¸°í™” ì™„ë£Œ")
        
    def setup_mobilenet(self):
        """MobileNet ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # MobileNetV3 Small (ê°€ë²¼ìš´ ëª¨ë¸)
            self.mobilenet = models.mobilenet_v3_small(pretrained=True)
            # íŠ¹ì§• ì¶”ì¶œê¸°ë¡œ ì‚¬ìš© (ë¶„ë¥˜ì¸µ ì œê±°)
            self.mobilenet.classifier = torch.nn.Identity()
            self.mobilenet.eval()
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            
            logger.info("âœ… MobileNet ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (~100MB)")
            
        except Exception as e:
            logger.error(f"âŒ MobileNet ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.mobilenet = None
    
    def extract_color_features(self, image):
        """ìƒ‰ìƒ íŠ¹ì§• ì¶”ì¶œ (Computer Vision)"""
        # RGB íˆìŠ¤í† ê·¸ë¨
        color_features = []
        for i in range(3):  # R, G, B
            hist = cv2.calcHist([image], [i], None, [32], [0, 256])
            color_features.extend(hist.flatten())
        
        # HSV íˆìŠ¤í† ê·¸ë¨ (ìƒ‰ìƒ ë§¤ì¹­ì— ë” íš¨ê³¼ì )
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        h_hist = cv2.calcHist([hsv], [0], None, [30], [0, 180])  # ìƒ‰ì¡°
        s_hist = cv2.calcHist([hsv], [1], None, [32], [0, 256])  # ì±„ë„
        
        color_features.extend(h_hist.flatten())
        color_features.extend(s_hist.flatten())
        
        return np.array(color_features)
    
    def extract_texture_features(self, image):
        """í…ìŠ¤ì²˜/íŒ¨í„´ íŠ¹ì§• ì¶”ì¶œ (Computer Vision)"""
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        
        # 1. LBP (Local Binary Pattern) - í…ìŠ¤ì²˜ ë¶„ì„
        lbp = self.calculate_lbp(gray)
        
        # 2. ì—£ì§€ íˆìŠ¤í† ê·¸ë¨ - íŒ¨í„´ ë¶„ì„
        edges = cv2.Canny(gray, 50, 150)
        edge_hist = cv2.calcHist([edges], [0], None, [16], [0, 256])
        
        # 3. ê·¸ë˜ë””ì–¸íŠ¸ ë°©í–¥ íˆìŠ¤í† ê·¸ë¨
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        grad_mag = np.sqrt(grad_x**2 + grad_y**2)
        grad_hist = cv2.calcHist([grad_mag.astype(np.uint8)], [0], None, [16], [0, 256])
        
        texture_features = np.concatenate([
            lbp.flatten(),
            edge_hist.flatten(), 
            grad_hist.flatten()
        ])
        
        return texture_features
    
    def calculate_lbp(self, gray_image, radius=1, n_points=8):
        """Local Binary Pattern ê³„ì‚°"""
        h, w = gray_image.shape
        lbp_image = np.zeros_like(gray_image)
        
        for i in range(radius, h - radius):
            for j in range(radius, w - radius):
                center = gray_image[i, j]
                
                # 8ë°©í–¥ ì´ì›ƒ í”½ì…€ê³¼ ë¹„êµ
                neighbors = [
                    gray_image[i-1, j-1], gray_image[i-1, j], gray_image[i-1, j+1],
                    gray_image[i, j+1], gray_image[i+1, j+1], gray_image[i+1, j],
                    gray_image[i+1, j-1], gray_image[i, j-1]
                ]
                
                # ì´ì§„ íŒ¨í„´ ìƒì„±
                binary_value = 0
                for k, neighbor in enumerate(neighbors):
                    if neighbor >= center:
                        binary_value += 2**k
                
                lbp_image[i, j] = binary_value
        
        # LBP íˆìŠ¤í† ê·¸ë¨
        lbp_hist = cv2.calcHist([lbp_image], [0], None, [32], [0, 256])
        return lbp_hist
    
    def extract_mobilenet_features(self, image):
        """MobileNetìœ¼ë¡œ ê³ ìˆ˜ì¤€ íŠ¹ì§• ì¶”ì¶œ"""
        if self.mobilenet is None:
            return np.zeros(576)  # ê¸°ë³¸ í¬ê¸° ë°˜í™˜
        
        try:
            # PIL Imageë¡œ ë³€í™˜
            if isinstance(image, np.ndarray):
                pil_image = Image.fromarray(image)
            else:
                pil_image = image
            
            # ì „ì²˜ë¦¬ ë° ì¶”ë¡ 
            input_tensor = self.transform(pil_image).unsqueeze(0)
            
            with torch.no_grad():
                features = self.mobilenet(input_tensor)
                # L2 ì •ê·œí™”
                features = features / (torch.norm(features, dim=1, keepdim=True) + 1e-8)
            
            return features.cpu().numpy().flatten()
            
        except Exception as e:
            logger.error(f"MobileNet íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros(576)
    
    def extract_hybrid_features(self, image):
        """Hybrid íŠ¹ì§• ì¶”ì¶œ: CV + MobileNet"""
        # 1. Computer Vision íŠ¹ì§•
        color_features = self.extract_color_features(image)
        texture_features = self.extract_texture_features(image)
        cv_features = np.concatenate([color_features, texture_features])
        
        # 2. MobileNet íŠ¹ì§•  
        mobilenet_features = self.extract_mobilenet_features(image)
        
        # 3. íŠ¹ì§• í¬ê¸° ì •ê·œí™”
        cv_size = 512
        mobilenet_size = 256
        
        if len(cv_features) > cv_size:
            cv_features = cv_features[:cv_size]
        elif len(cv_features) < cv_size:
            cv_features = np.pad(cv_features, (0, cv_size - len(cv_features)))
            
        if len(mobilenet_features) > mobilenet_size:
            mobilenet_features = mobilenet_features[:mobilenet_size]
        elif len(mobilenet_features) < mobilenet_size:
            mobilenet_features = np.pad(mobilenet_features, (0, mobilenet_size - len(mobilenet_features)))
        
        # 4. ê°€ì¤‘ ê²°í•© (CV 70%, MobileNet 30%)
        cv_weight = 0.7
        mobilenet_weight = 0.3
        
        hybrid_features = np.concatenate([
            cv_features * cv_weight,
            mobilenet_features * mobilenet_weight
        ])
        
        # 5. L2 ì •ê·œí™”
        hybrid_features = hybrid_features / (np.linalg.norm(hybrid_features) + 1e-8)
        
        return hybrid_features
    
    def register_suspect(self, suspect_id: str, clothing_image: np.ndarray) -> Dict[str, Any]:
        """ìš©ì˜ì ì˜·ì°¨ë¦¼ ë“±ë¡"""
        try:
            # íŠ¹ì§• ì¶”ì¶œ
            features = self.extract_hybrid_features(clothing_image)
            
            # ë“±ë¡ ì •ë³´ ì €ì¥
            self.registered_suspects[suspect_id] = {
                "features": features.tolist(),
                "feature_size": len(features),
                "registration_method": "hybrid_cv_mobilenet"
            }
            
            logger.info(f"âœ… ìš©ì˜ì ë“±ë¡ ì™„ë£Œ: {suspect_id}")
            
            return {
                "status": "success",
                "suspect_id": suspect_id,
                "feature_dimension": len(features),
                "method": "hybrid_cv_mobilenet",
                "message": f"ìš©ì˜ì '{suspect_id}' ì˜·ì°¨ë¦¼ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤"
            }
            
        except Exception as e:
            logger.error(f"âŒ ìš©ì˜ì ë“±ë¡ ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "message": f"ë“±ë¡ ì‹¤íŒ¨: {str(e)}"
            }
    
    def match_clothing(self, query_image: np.ndarray, threshold: float = 0.7) -> Dict[str, Any]:
        """CCTV ì´ë¯¸ì§€ì™€ ë“±ë¡ëœ ì˜·ì°¨ë¦¼ ë§¤ì¹­"""
        try:
            if not self.registered_suspects:
                return {
                    "status": "no_suspects",
                    "message": "ë“±ë¡ëœ ìš©ì˜ìê°€ ì—†ìŠµë‹ˆë‹¤",
                    "matches": []
                }
            
            # ì¿¼ë¦¬ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
            query_features = self.extract_hybrid_features(query_image)
            
            matches = []
            for suspect_id, suspect_data in self.registered_suspects.items():
                stored_features = np.array(suspect_data["features"])
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = cosine_similarity(
                    query_features.reshape(1, -1),
                    stored_features.reshape(1, -1)
                )[0][0]
                
                if similarity >= threshold:
                    confidence = "high" if similarity > 0.8 else "medium"
                    
                    matches.append({
                        "suspect_id": suspect_id,
                        "similarity": float(similarity),
                        "confidence": confidence,
                        "similarity_percentage": float(similarity * 100),
                        "match": True
                    })
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            matches.sort(key=lambda x: x["similarity"], reverse=True)
            
            logger.info(f"ë§¤ì¹­ ì™„ë£Œ: {len(matches)}ëª… ë°œê²¬")
            
            return {
                "status": "success",
                "total_comparisons": len(self.registered_suspects),
                "matches_found": len(matches),
                "threshold": threshold,
                "matches": matches,
                "method": "hybrid_cv_mobilenet"
            }
            
        except Exception as e:
            logger.error(f"âŒ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "message": f"ë§¤ì¹­ ì‹¤íŒ¨: {str(e)}",
                "matches": []
            }
    
    def get_registered_suspects(self) -> Dict[str, Any]:
        """ë“±ë¡ëœ ìš©ì˜ì ëª©ë¡ ì¡°íšŒ"""
        return {
            "status": "success",
            "total_suspects": len(self.registered_suspects),
            "suspect_ids": list(self.registered_suspects.keys()),
            "method": "hybrid_cv_mobilenet"
        }
    
    def delete_suspect(self, suspect_id: str) -> Dict[str, Any]:
        """ìš©ì˜ì ì‚­ì œ"""
        if suspect_id in self.registered_suspects:
            del self.registered_suspects[suspect_id]
            return {
                "status": "success",
                "message": f"ìš©ì˜ì '{suspect_id}'ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤",
                "remaining_suspects": len(self.registered_suspects)
            }
        else:
            return {
                "status": "error",
                "message": f"ìš©ì˜ì '{suspect_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            }
    
    def analyze_clothing_details(self, image: np.ndarray) -> Dict[str, Any]:
        """ì˜·ì°¨ë¦¼ ìƒì„¸ ë¶„ì„ (ë””ë²„ê¹…/ì‹œì—°ìš©)"""
        try:
            # ìƒ‰ìƒ ë¶„ì„
            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
            
            # ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
            dominant_colors = self.get_dominant_colors(image)
            
            # í…ìŠ¤ì²˜ ë¶„ì„
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            texture_variance = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # íŒ¨í„´ ë¶„ì„
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
            
            return {
                "dominant_colors": dominant_colors,
                "texture_complexity": "high" if texture_variance > 500 else "medium" if texture_variance > 100 else "low",
                "pattern_density": "high" if edge_density > 0.1 else "medium" if edge_density > 0.05 else "low",
                "analysis_method": "hybrid_cv_analysis"
            }
            
        except Exception as e:
            logger.error(f"ì˜·ì°¨ë¦¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def get_dominant_colors(self, image: np.ndarray, k: int = 3) -> List[str]:
        """ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ"""
        try:
            # ì´ë¯¸ì§€ë¥¼ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜
            data = image.reshape((-1, 3))
            data = np.float32(data)
            
            # K-means í´ëŸ¬ìŠ¤í„°ë§
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
            _, labels, centers = cv2.kmeans(data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
            
            # ìƒ‰ìƒì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            colors = []
            for center in centers:
                b, g, r = center.astype(int)
                color_name = self.rgb_to_color_name(r, g, b)
                colors.append(color_name)
            
            return colors
            
        except Exception as e:
            return ["ì•Œ ìˆ˜ ì—†ìŒ"]
    
    def rgb_to_color_name(self, r: int, g: int, b: int) -> str:
        """RGB ê°’ì„ ìƒ‰ìƒ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        # ê°„ë‹¨í•œ ìƒ‰ìƒ ë¶„ë¥˜
        if r > 200 and g > 200 and b > 200:
            return "í°ìƒ‰"
        elif r < 50 and g < 50 and b < 50:
            return "ê²€ì€ìƒ‰"
        elif r > 150 and g < 100 and b < 100:
            return "ë¹¨ê°„ìƒ‰"
        elif r < 100 and g > 150 and b < 100:
            return "ì´ˆë¡ìƒ‰"
        elif r < 100 and g < 100 and b > 150:
            return "íŒŒë€ìƒ‰"
        elif r > 150 and g > 150 and b < 100:
            return "ë…¸ë€ìƒ‰"
        elif r > 150 and g < 100 and b > 150:
            return "ë³´ë¼ìƒ‰"
        elif r > 100 and g > 100 and b > 100:
            return "íšŒìƒ‰"
        else:
            return "ê¸°íƒ€ìƒ‰ìƒ"

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    matcher = HybridClothingMatcher()
    print("âœ… Hybrid Clothing Matcher í…ŒìŠ¤íŠ¸ ì™„ë£Œ")